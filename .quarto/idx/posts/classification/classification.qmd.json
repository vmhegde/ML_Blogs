{"title":"Classification - Classifying Texts in Different Languages","markdown":{"yaml":{"title":"Classification - Classifying Texts in Different Languages","jupyter":"python3"},"headingText":"Plot languages against the number of texts in that language","containsRefs":false,"markdown":"\n\n```{python}\n#| tags: []\nimport sys\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nassert sys.version_info >= (3, 7)\n```\n\n```{python}\n#| tags: []\nfrom packaging import version\nimport sklearn\n\nassert version.parse(sklearn.__version__) >= version.parse(\"1.0.1\")\n```\n\nFirst we read the input data. The data contains two columns: Text and Language\n\n```{python}\n#| tags: []\nfrom sklearn.datasets import fetch_openml\n\ndf = pd.read_csv(\"./Language Detection-checkpoint.csv\")\n```\n\n```{python}\n#| tags: []\nprint(df.keys())\ndf['Text']\n```\n\n\n```{python}\nplt.rc('font', size=14)\nplt.rc('axes', labelsize=14, titlesize=14)\nplt.rc('legend', fontsize=14)\nplt.rc('xtick', labelsize=10)\nplt.rc('ytick', labelsize=10)\n\nplt.bar(x=df['Language'].value_counts().index, height=df['Language'].value_counts())\nplt.xlabel('Language')\nplt.ylabel('Number of Texts')\nplt.xticks(rotation=90)\nplt.show()\n```\n\n## Preprocess the Data\n\nMake sure the texts only contain alphabetical characters\n\n```{python}\n# Text data preprocessing\n# Uniform case, remove symbols and whitespace\ndf['Text'] = df['Text'].str.lower()\ndf['Text'] = df['Text'].str.replace(r'[\\([{})\\]!@#$,\"%^*?:;~`0-9]', '', regex=True)\ndf['Text'] = df['Text'].str.strip()\n```\n\n## Using TF-IDF: Term Frequency Inverse Document Frequency\n\nWe use TF-IDF to transform text into numeric data. This measures the originality of a word by comparing the number of times a particular word appears in a text versus the number of texts that word appears in\n\n```{python}\n#| tags: []\n# The TF-IDF vectorizer initializing \nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_vectorizer = TfidfVectorizer()\ntfidf_vectorizer.fit(df['Text'])\nX_tfidf = tfidf_vectorizer.transform(df['Text'])\n```\n\nWe use a Label Encoder to convert each language name to a number.\n\n```{python}\n#| tags: []\n# Encoding the target labels (languages)\nfrom sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(df['Language'])\n```\n\n## Split the data into testing and training sets\n\n```{python}\n#| tags: []\nX_train, X_test, y_train, y_test = X_tfidf[:5000], X_tfidf[5000:], y_encoded[:5000], y_encoded[5000:]\n#X_train, X_test, y_train, y_test = X_tfidf[:5000], X_tfidf[5000:10000], y_num[:5000], y_num[5000:10000]\nX_train_english = (X_train == 1) # True for all English texts, English = 1\n\nprint(y_encoded)\n```\n\n## Use SGD (Stochastic Gradient Descent) to classify and make predictions\n\n```{python}\n# Training binary classifier\nfrom sklearn.linear_model import SGDClassifier\nimport numpy as np\n\nsgd_clf = SGDClassifier(random_state=42)\nsgd_clf.fit(X_train, y_train)\n```\n\n## Prediction Function\n\nThis prediction function takes a text as input, and outputs the predicted language that the text is written in.\n\n```{python}\n#| tags: []\n# predict languge given text \ndef predict_language(input_text):\n    input_text = input_text.lower() # preprocess input text\n    input_tfidf = tfidf_vectorizer.transform([input_text]) # convert input to numeric data\n    predicted = sgd_clf.predict(input_tfidf) # get encoded prediction\n    predicted_language = label_encoder.inverse_transform(predicted) # decode prediction\n    return predicted_language\n\n#predict_language(X[1400])\n```\n\n## Data Visualization\n\nCreating a confusion matrix to illustrate the predictions made by the classifier.\n\n```{python}\n#| tags: []\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import LabelEncoder\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Fit the label encoder on all possible labels\nlabel_encoder = LabelEncoder()\nlabel_encoder.fit(df['Language'])  # Fit on the entire label set\n\n# Now transform the test labels\nLanguage_test_encoded = label_encoder.transform(df['Language'][5000:])\n\nText_test = df['Text'][5000:]\npredicted_languages = [predict_language(text) for text in Text_test]\n\n# Transform the predicted languages back to the encoded form\npredicted_languages_encoded = label_encoder.transform(predicted_languages)\n\n# Comparing the predicted languages with the actual labels\nresults_df = pd.DataFrame({'Actual': Language_test_encoded, 'Predicted': predicted_languages_encoded})\n\n# Construct the confusion matrix manually\nconf_matrix = confusion_matrix(results_df['Actual'], results_df['Predicted'])\n# Get all unique labels present in the actual and predicted labels\nall_labels = np.union1d(results_df['Actual'], results_df['Predicted'])\n\n# Convert to DataFrame for easier plotting\n# Use all_labels for both the index and columns to account for every possible class\nconf_matrix_df = pd.DataFrame(conf_matrix, \n                              index=all_labels, \n                              columns=all_labels)\n\n# Plot the confusion matrix\nplt.figure(figsize=(10, 8))\nsns.heatmap(conf_matrix_df, annot=True, fmt='d', square=True)\nplt.title('Confusion Matrix of Language Predictions')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.show()\n\n# Calculate accuracy\naccuracy = (results_df['Actual'] == results_df['Predicted']).mean()\nprint(f'Accuracy of the predict_language function: {accuracy:.2f}')\n```\n\n## Interpretation\n\nFrom the confusion matrix, we can see that the model is very good at predicting some languages (like the one labeled '8', with 341 correct predictions). Some languages are often confused with others, such as labels '5', '6', '9', and '12', indicating potential areas for model improvement. The accuracy of this model could potentially be improved by using more training data and by trying different algorithms.\n","srcMarkdownNoYaml":"\n\n```{python}\n#| tags: []\nimport sys\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nassert sys.version_info >= (3, 7)\n```\n\n```{python}\n#| tags: []\nfrom packaging import version\nimport sklearn\n\nassert version.parse(sklearn.__version__) >= version.parse(\"1.0.1\")\n```\n\nFirst we read the input data. The data contains two columns: Text and Language\n\n```{python}\n#| tags: []\nfrom sklearn.datasets import fetch_openml\n\ndf = pd.read_csv(\"./Language Detection-checkpoint.csv\")\n```\n\n```{python}\n#| tags: []\nprint(df.keys())\ndf['Text']\n```\n\n## Plot languages against the number of texts in that language\n\n```{python}\nplt.rc('font', size=14)\nplt.rc('axes', labelsize=14, titlesize=14)\nplt.rc('legend', fontsize=14)\nplt.rc('xtick', labelsize=10)\nplt.rc('ytick', labelsize=10)\n\nplt.bar(x=df['Language'].value_counts().index, height=df['Language'].value_counts())\nplt.xlabel('Language')\nplt.ylabel('Number of Texts')\nplt.xticks(rotation=90)\nplt.show()\n```\n\n## Preprocess the Data\n\nMake sure the texts only contain alphabetical characters\n\n```{python}\n# Text data preprocessing\n# Uniform case, remove symbols and whitespace\ndf['Text'] = df['Text'].str.lower()\ndf['Text'] = df['Text'].str.replace(r'[\\([{})\\]!@#$,\"%^*?:;~`0-9]', '', regex=True)\ndf['Text'] = df['Text'].str.strip()\n```\n\n## Using TF-IDF: Term Frequency Inverse Document Frequency\n\nWe use TF-IDF to transform text into numeric data. This measures the originality of a word by comparing the number of times a particular word appears in a text versus the number of texts that word appears in\n\n```{python}\n#| tags: []\n# The TF-IDF vectorizer initializing \nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_vectorizer = TfidfVectorizer()\ntfidf_vectorizer.fit(df['Text'])\nX_tfidf = tfidf_vectorizer.transform(df['Text'])\n```\n\nWe use a Label Encoder to convert each language name to a number.\n\n```{python}\n#| tags: []\n# Encoding the target labels (languages)\nfrom sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(df['Language'])\n```\n\n## Split the data into testing and training sets\n\n```{python}\n#| tags: []\nX_train, X_test, y_train, y_test = X_tfidf[:5000], X_tfidf[5000:], y_encoded[:5000], y_encoded[5000:]\n#X_train, X_test, y_train, y_test = X_tfidf[:5000], X_tfidf[5000:10000], y_num[:5000], y_num[5000:10000]\nX_train_english = (X_train == 1) # True for all English texts, English = 1\n\nprint(y_encoded)\n```\n\n## Use SGD (Stochastic Gradient Descent) to classify and make predictions\n\n```{python}\n# Training binary classifier\nfrom sklearn.linear_model import SGDClassifier\nimport numpy as np\n\nsgd_clf = SGDClassifier(random_state=42)\nsgd_clf.fit(X_train, y_train)\n```\n\n## Prediction Function\n\nThis prediction function takes a text as input, and outputs the predicted language that the text is written in.\n\n```{python}\n#| tags: []\n# predict languge given text \ndef predict_language(input_text):\n    input_text = input_text.lower() # preprocess input text\n    input_tfidf = tfidf_vectorizer.transform([input_text]) # convert input to numeric data\n    predicted = sgd_clf.predict(input_tfidf) # get encoded prediction\n    predicted_language = label_encoder.inverse_transform(predicted) # decode prediction\n    return predicted_language\n\n#predict_language(X[1400])\n```\n\n## Data Visualization\n\nCreating a confusion matrix to illustrate the predictions made by the classifier.\n\n```{python}\n#| tags: []\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import LabelEncoder\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Fit the label encoder on all possible labels\nlabel_encoder = LabelEncoder()\nlabel_encoder.fit(df['Language'])  # Fit on the entire label set\n\n# Now transform the test labels\nLanguage_test_encoded = label_encoder.transform(df['Language'][5000:])\n\nText_test = df['Text'][5000:]\npredicted_languages = [predict_language(text) for text in Text_test]\n\n# Transform the predicted languages back to the encoded form\npredicted_languages_encoded = label_encoder.transform(predicted_languages)\n\n# Comparing the predicted languages with the actual labels\nresults_df = pd.DataFrame({'Actual': Language_test_encoded, 'Predicted': predicted_languages_encoded})\n\n# Construct the confusion matrix manually\nconf_matrix = confusion_matrix(results_df['Actual'], results_df['Predicted'])\n# Get all unique labels present in the actual and predicted labels\nall_labels = np.union1d(results_df['Actual'], results_df['Predicted'])\n\n# Convert to DataFrame for easier plotting\n# Use all_labels for both the index and columns to account for every possible class\nconf_matrix_df = pd.DataFrame(conf_matrix, \n                              index=all_labels, \n                              columns=all_labels)\n\n# Plot the confusion matrix\nplt.figure(figsize=(10, 8))\nsns.heatmap(conf_matrix_df, annot=True, fmt='d', square=True)\nplt.title('Confusion Matrix of Language Predictions')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.show()\n\n# Calculate accuracy\naccuracy = (results_df['Actual'] == results_df['Predicted']).mean()\nprint(f'Accuracy of the predict_language function: {accuracy:.2f}')\n```\n\n## Interpretation\n\nFrom the confusion matrix, we can see that the model is very good at predicting some languages (like the one labeled '8', with 341 correct predictions). Some languages are often confused with others, such as labels '5', '6', '9', and '12', indicating potential areas for model improvement. The accuracy of this model could potentially be improved by using more training data and by trying different algorithms.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"classification.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","editor":"visual","theme":"cosmo","title-block-banner":true,"title":"Classification - Classifying Texts in Different Languages","jupyter":"python3"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}